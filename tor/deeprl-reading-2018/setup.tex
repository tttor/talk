\section{Setup}
\frame{\tableofcontents[currentsection, hideothersubsections]}

\begin{frame}
\frametitle{Experiment Setup: Task}

\begin{figure}
    \centering
    \includegraphics[scale=0.3]{fig_1}
\end{figure}

\begin{itemize}
  \item 20 simulated physics task, including:
  \begin{itemize}
    \item gripperRandom:
    Agent must use an arm with gripper appendage to grasp an object and manuver the object, which
    are initialized in random locations
    % \item reacher3daRandomTarget:
    % Agent is required to move a 7-DOF human-like arm from random starting locations to random target positions.
    \item reacherObstacle:
    Agent is required to move a 5-DOF arm around an obstacle to a randomized target position.
  \end{itemize}
  \item using both a low-dimensional state description (such as joint angles and positions) and high-dimensional renderings of the environment
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Experiment Setup: Task}

\begin{figure}
    \centering
    \includegraphics[scale=0.25]{task_dim}
\end{figure}

\end{frame}


\begin{frame}
\frametitle{Experiment Setup: Baseline, Assumptions}

Assumption:
\begin{itemize}
  \item env is MDP, the environment is fully-observed
\end{itemize}

Baseline:
\begin{itemize}
  \item types:
  \begin{itemize}
    \item naive policy: the mean return from a naive policy which samples actions from
    a uniform distribution over the valid action space;
    \item iLQG: a planning based solver with full access to
    the underlying physical model and its derivatives.
  \end{itemize}
  \item normalize scores so that
    \begin{itemize}
    \item naive policy: has a mean score of 0
    \item iLQG: has a mean score of 1
  \end{itemize}
\end{itemize}

\end{frame}

