\section{Background}
\frame{\tableofcontents[currentsection, hideothersubsections]}

\begin{frame}
\frametitle{MDP here}
\begin{itemize}
  \item state space, $S$
  \item action space, $A = R^N$
  \item an initial state distribution $p(s_1)$
  \item transition dynamics
  \item reward fn
  \item return as the sum of discounted future reward
  \item $\gamma \in [0,1]$
  \item goal: to learn a policy which maximizes the expected return from
  the start distribution $J = E[R_1]$
  \item $Q^{\phi}(s_t,a_t) = ...$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Deterministic}
Bellman equation (the recursive relationship):
\begin{equation}
Q^{\phi} (s_t,a_t) = \mathbb{E} \Big[ r(s_t,a_t) + \gamma \mathbb{E} [Q^{\phi}(s_{t+1},a_{t+1})] \Big]
\end{equation}

Having a deterministic target policy, $\mu: S \mapsto A$:
\begin{equation}
Q^{\phi} (s_t,a_t) = \mathbb{E} \Big[ r(s_t,a_t) + \gamma \mathbb{E} [Q^{\mu}(s_{t+1},\mu(s_{t+1}))] \Big]
\end{equation}
which:
avoid the inner expectation, i.e.
the expectation depends only on the environment,

Thus: possible to learn $Q^{\mu}$ off-policy,
using transitions which are generated from a different stochastic behavior policy $\beta$.
commonly: $\mu(s) = argmax_a Q(s,a)$ as in Q-learning
\end{frame}

\begin{frame}
\frametitle{func approx}
consider function approximators parameterized by $\theta^Q$, which we optimize by minimizing the loss:
\begin{equation*}
L(\theta^Q) =\mathbb{E} \Big[ Q^{\phi} (s_t,a_t|\theta^Q) - y_t \Big]
\end{equation*}
where $y_t = r(s_t,a_t) + \gamma Q^{\mu}(s_{t+1},\mu(s_{t+1}) | \theta^Q)$.

Typically, ignore the fact that $y_t$ is also dependent on $\theta^Q$.

\end{frame}


\begin{frame}
\frametitle{Background: Deep Q-network}
deep Q-learning \cite{Mnih2013}
\begin{itemize}
  \item innovation:
  \begin{itemize}
    \item the network is trained off-policy with samples from a replay buffer to minimize correlations between samples;
    the use of a replay buffer
    \item the network is trained with a target Q network to give consistent targets during temporal difference backups.
    a separate target network for calculating $y_t$
  \end{itemize}
  \item able to:
  \begin{itemize}
    \item solves problems with high-dimensional observation spaces,
    \item can only handle discrete and low-dimensional action spaces.
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Background: Deterministic policy gradient (DPG) \cite{Silver2014}}

DPG:
\begin{itemize}
  \item maintains a parameterized actor function $\mu (s|\theta^{\mu})$ which
  specifies the current policy by deterministically mapping states to a specific action
  \item The critic $Q(s, a)$ is learned using the Bellman equation as in Q-learning.
\end{itemize}

Key:
\begin{itemize}
  \item replay buffer: a finite sized cache $R$,
  \item target network:
  \item batch normalization: normalizes each dimension across the samples
  in a minibatch to have unit mean and variance.
  \item an exploration policy $Âµ'$ by adding noise sampled from
  a noise (Ornstein-Uhlenbeck process) process $\mathcal{N}$ to our actor policy
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Background: Actor-critic \cite{Sutton1998}}
The Actor-Critic Algorithm is essentially a hybrid method to combine the policy gradient method and the value function method together.
The policy function is known as the actor, while the value function is referred to as the critic.
Essentially, the actor produces the action aa given the current state of the environment ss, while the critic produces a signal to criticizes the actions made by the actor.
\end{frame}


