\section{Background}
\frame{\tableofcontents[currentsection, hideothersubsections]}

\begin{frame}
\frametitle{Background: MDP}
\textbf{The environment}, $E$: \\
a Markov decision process with a state space $S$, action space $A = \mathbb{R}^N$,
an initial state distribution $p(s_1)$, transition dynamics $p(s_{t+1}|s_t, a_t)$, and
reward function $r(s_t, a_t)$.
\vspace{2.5mm}

\textbf{At each discrete timestep} $t$, \\
the agent receives an observation $x_t = s_t$,
takes an action $a_t$ and receives a scalar reward $r_t$.
\vspace{2.5mm}

\textbf{The return from a state}: \\
$R_t= \sum_{i=t}^T  \gamma^{(i-t)} r(s_i, a_i)$ with a discounting factor $\gamma \in [0, 1]$.
\vspace{2.5mm}

\textbf{Goal}: \\
to learn a policy, $\pi: S \mapsto P(A)$, which
maximizes $J = \mathbb{E}_{r_i,s_i \sim E,a_i \sim \pi} [R_1]$.
\vspace{2.5mm}

\textbf{Action-value function}:\\
$Q^{\pi}(s_t,a_t) = \mathbb{E}_{r_{i \ge t},s_{i>t} \sim E,a_{i>t} \sim \pi} [R_t|s_t,a_t]$.
\end{frame}

\begin{frame}
\frametitle{Background: MDP with deterministic policy}
Bellman equation (the recursive relationship):
\begin{equation}
Q^{\pi} (s_t,a_t) = \mathbb{E}_{r_{t},s_{t+1} \sim E} \Big[ r(s_t,a_t) + \gamma \mathbb{E}_{a_{t+1} \sim \pi} [Q^{\phi}(s_{t+1},a_{t+1})] \Big]
\end{equation}

If the target policy is deterministic, $\mu: S \mapsto A$, then:
\begin{equation}
Q^{\mu} (s_t,a_t) = \mathbb{E}_{r_{t},s_{t+1} \sim E} \Big[ r(s_t,a_t) + \gamma Q^{\mu}(s_{t+1},\mu(s_{t+1})) \Big]
\end{equation}

Thus:
\begin{itemize}
\item the expectation depends only on the environment $E$,
\item possible to learn $Q^{\mu}$ off-policy, using transitions which
are generated from a different stochastic behavior policy $\beta$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Background: Fn approximator}
Consider fn approximators parameterized by $\theta^Q$, \\
which we optimize by minimizing the loss:
\begin{equation*}
L(\theta^Q) =\mathbb{E}_{s_t \sim \rho^{\beta}, a \sim \beta, r_t \sim E} \Big[ \Big( Q^{\phi} (s_t,a_t|\theta^Q) - y_t \Big)^2 \Big]
\end{equation*}
where
$y_t = r(s_t,a_t) + \gamma Q^{\mu}(s_{t+1},\mu(s_{t+1}) | \theta^Q)$, and
$\rho^{\beta}$: the discounted state visitation distribution for a policy $\beta$.

Typically, ignore the fact that $y_t$ is also dependent on $\theta^Q$.\\

\end{frame}


\begin{frame}
\frametitle{Background: Deep Q-network \cite{Mnih2013}}

Key:
\begin{itemize}
  \item replay buffer,
  contain $(s_t, a_t, r_t, s_{t+1})$ sampled from $E$ using an exploration policy
  \item target network:
  \item batch normalization: normalizes each dimension across the samples
  in a minibatch to have unit mean and variance.
  \item an exploration policy $Âµ'$ by adding noise sampled from
  a noise (Ornstein-Uhlenbeck process) process $\mathcal{N}$ to our actor policy
\end{itemize}

% At each timestep the actor and critic are updated by sampling a minibatch uniformly from the buffer.
% Because DDPG is an off-policy algorithm, the replay buffer can be large, allowing the algorithm to benefit from learning across a set of uncorrelated transitions.

% deep Q-network:
% \begin{itemize}
%   \item innovation:
%   \begin{itemize}
%     \item the network is trained off-policy with samples from a replay buffer to minimize correlations between samples;
%     the use of a replay buffer
%     \item the network is trained with a target Q network to give consistent targets during temporal difference backups.
%     a separate target network for calculating $y_t$
%   \end{itemize}
%   \item able to:
%   \begin{itemize}
%     \item solves problems with high-dimensional observation spaces,
%     \item can only handle discrete and low-dimensional action spaces.
%   \end{itemize}
% \end{itemize}

\end{frame}

\begin{frame}
\frametitle{Background: DPG \cite{Silver2014}}

DPG: Deterministic Policy Gradient, an actor-critic approach:
\begin{itemize}
  \item actor: $\mu (s|\theta^{\mu})$,
  specifies the current policy by deterministically mapping states to a specific action
  \item critic: $Q(s, a)$,
  learned using the Bellman equation as in Q-learning.
\end{itemize}


Update $\theta^{\mu}$ by applying the chain rule to the expected return from
the start distribution, $J$, with respect to the actor parameters, $\theta^{\mu}$:
\begin{equation} \label{equ:policy_grad}
\begin{split}
\nabla_{\theta^{\mu}} J &  \approx \mathbb{E}_{s_t \sim \rho^{\beta}} \Big[ \nabla_{\theta^{\mu}} Q(s,a|\theta^Q) |_{s = s_t, a = \mu(s_t|\theta^{\mu})} \Big] \\
  & = \mathbb{E}_{s_t \sim \rho^{\beta}} \Big[ \nabla_{\theta^{\mu}} Q(s,a|\theta^Q) |_{s = s_t, a = \mu(s_t)} \nabla_{\theta^{\mu}} \mu(s|\theta^{\mu})|_{s = s_t} \Big]
\end{split}
\end{equation}

Equ.~\ref{equ:policy_grad} is the policy gradient (the gradient of the policy's performance),
in practice the discount in $\rho^{\beta}$ is ignored.

\end{frame}

\begin{frame}
\frametitle{Background: Actor-critic \cite{Sutton1998}}
The Actor-Critic Algorithm is essentially a hybrid method to combine the policy gradient method and the value function method together.
The policy function is known as the actor, while the value function is referred to as the critic.
Essentially, the actor produces the action aa given the current state of the environment ss, while the critic produces a signal to criticizes the actions made by the actor.
\end{frame}


