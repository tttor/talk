\section{Conclusions}
\frame{\tableofcontents[currentsection, hideothersubsections]}

\begin{frame}
\frametitle{Limitation}
\begin{itemize}
  \item DDPG requires a large number of training episodes to find solutions
  (as with most model-free reinforcement approaches)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Follow up}
\begin{itemize}
\item https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html
\item Benchmarking Deep Reinforcement Learning for Continuous Control
\item DeepMind Control Suite
\item Data-efficient Deep Reinforcement Learning for Dexterous Manipulation
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conclusions}

\end{frame}

\begin{frame}
\Huge{\centerline{Discussion time and thank you.}}
\end{frame}

% Deep DPG:
% DPG that uses neural network function approximators to
% learn in large state and action spaces online.
% \begin{itemize}
%   \item a model-free, off-policy actor-critic algorithm using deep function approximators
%   \item can learn competitive policies for all of our tasks using low-dimensional observations
%   (e.g. cartesian coordinates or joint angles) using the same hyper-parameters and network structure.
%   \item In many cases, we are also able to learn good policies directly from pixels, again keeping hyperparameters and network structure constant
% \end{itemize}
