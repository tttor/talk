\section{Conclusions}
\frame{\tableofcontents[currentsection, hideothersubsections]}

\begin{frame}
\frametitle{Conclusions}
\textbf{Deep DPG:} DPG that uses neural network function approximators to
learn in large state and action spaces online.
\vspace{3mm}

\textbf{Limitations:}
\begin{itemize}
  \item DDPG requires a large number of training episodes to find solutions
  (as with most model-free reinforcement approaches)
  \item assume fully observability
\end{itemize}
\vspace{3mm}

\textbf{Follow up:}
\begin{itemize}
\item {\footnotesize Data-efficient Deep Reinforcement Learning for Dexterous Manipulation}
\item Benchmarking Deep Reinforcement Learning for Continuous Control
\item DeepMind Control Suite
\item \textbf{RL Brotherhood @UQ:}
reading, brainstroming, peerReview,
reproducing others' work, code base, counseling, pizza :)
% \item https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html
\end{itemize}

\end{frame}

% \begin{itemize}
%   \item a model-free, off-policy actor-critic algorithm using deep function approximators
%   \item can learn competitive policies for all of our tasks using low-dimensional observations
%   (e.g. cartesian coordinates or joint angles) using the same hyper-parameters and network structure.
%   \item In many cases, we are also able to learn good policies directly from pixels, again keeping hyperparameters and network structure constant
% \end{itemize}
