\section{Introduction}
\frame{\tableofcontents[currentsection, hideothersubsections]}

\begin{frame}
\frametitle{Introduction}
Main:\\
Continuous control with deep reinforcement learning~\cite{Lillicrap2015}
\vspace{2mm}

Problems:
\begin{itemize}
  \item continuous action space
  \item raw pixels for observations
\end{itemize}

DDPG (=DPG+DQN) components:
\begin{itemize}
  \item Deterministic Policy-Gradient (DPG) \cite{Silver2014}
  \item Deep Q-Network (DQN) \cite{Mnih2013}
  \item Actor-Critic Methods \cite{Sutton1998}
\end{itemize}

% Common misconception about Deep Learnig:
% \begin{itemize}
%   \item spend days to tune hyperparams:
%   Silver: false, we use our first arch choice, and
%   the considered it as a black-box powerful fn approx in most deepRL tasks
%   \item local minima traps:
%   Silver: in high dim param space, most local minima have values almost as good as global minima,
%   the more param you have, you should be less worry
%   \item we do not really know what is going on and no theoretical proof
%   Ng: DL is essentially NN with solid math, and yes, math in DL is more sophisticated,
%   speaking of empirical evidence, it is acceptable for
% \end{itemize}

\end{frame}
