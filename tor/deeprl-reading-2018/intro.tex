\section{Introduction}
\frame{\tableofcontents[currentsection, hideothersubsections]}

\begin{frame}
\frametitle{Problems}
\begin{itemize}
  \item continuous action space
  \item raw pixels for observations
\end{itemize}

DDPG components:
\begin{itemize}
  \item Deterministic Policy-Gradient Algorithms \cite{Silver2014}
  \item Actor-Critic Methods \cite{Sutton1998}
  \item Deep Q-Network \cite{Mnih2013}
\end{itemize}

Common misconception about Deep Learnig:
\begin{itemize}
  \item spend days to tune hyperparams:
  Silver: false, we use our first arch choice, and
  the considered it as a black-box powerful fn approx in most deepRL tasks
  \item local minima traps:
  Silver: in high dim param space, most local minima have values almost as good as global minima,
  the more param you have, you should be less worry
  \item we do not really know what is going on and no theoretical proof
  Ng: DL is essentially NN with solid math, and yes, math in DL is more sophisticated,
  speaking of empirical evidence, it is acceptable for
\end{itemize}

\end{frame}
