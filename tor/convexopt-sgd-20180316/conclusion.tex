\section{Conclusions}
\frame{\tableofcontents[currentsection, hideothersubsections]}

\begin{frame}
\frametitle{Conclusions}

\begin{itemize}
    \item SGD can directly minimize the risk function
    \item by sampling a point i.i.d from D and using a subgradient of the loss of the
        current hypothesis w(t) at this point as an unbiased estimate of the gradient (or
        a subgradient) of the risk function.
    \item This implies that a bound on the number of iterations also yields a sample complexity bound.
\end{itemize}
%  We have analyzed their convergence
% rate and calculated the number of iterations that would guarantee an expected
% objective of at most  plus the optimal objective.

WARN: Skipped (Subsub)sections:\\
\begin{itemize}
    \item any proofs
    \item 14.2.2 Subgradients of Lipschitz Functions
    \item 14.4 Variants
\end{itemize}

\end{frame}

