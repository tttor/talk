\section{Conclusions}
\frame{\tableofcontents[currentsection, hideothersubsections]}

\begin{frame}
\frametitle{Conclusions}

SGD can directly minimize the risk function
\begin{itemize}
    \item by sampling a point i.i.d from D \textbf{and}
    \item using a subgradient of the loss of the current hypothesis at this point as
    an unbiased estimate of the (sub)gradient of the risk function.
\end{itemize}
\vspace{5mm}

SGD's \textbf{convergence rate} and \textbf{the number of iterations} \\
would guarantee an expected objective of at most $\epsilon$ plus the optimal objective.
\vspace{5mm}

WARN: Skipped (Subsub)sections:\\
\begin{itemize}
    \item any proofs
    \item 14.2.2 Subgradients of Lipschitz Functions
    \item 14.4 Variants
\end{itemize}

\end{frame}

